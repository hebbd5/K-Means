{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means with Scikit-Learn\n",
    "Although we can build this algorithm from scratch, the good news here is that, most of the time, we won't need to. The scikit-learn library is a very powerful machine learning library for Python. Among many other algorithms, sklearn also has its complete, ready-to-use implementation of KMeans, which we'll use from now on.\n",
    "\n",
    "It's important to note, however, that any data scientist should understand the algorithms they use, which is why it's valuable to know how to write K-means from scratch.\n",
    "\n",
    "`scikit-learn`'s K-means has a few noteworthy features. First of all, it's very efficient — it takes advantage of much faster data structures than a Pandas DataFrame, for example.\n",
    "\n",
    "The most important aspects of scikit-learn's implementations are the following:\n",
    "\n",
    "- It's not restricted to two-column datasets.\n",
    "\n",
    "- It has the `n_init` parameter. This parameter represents the number of times the entire algorithm will initialize once you run it. As the first centroids are randomly initialized, there's always a chance they won't converge to the clusters with lower possible inertia. That's why scikit-learn uses this parameter to run the algorithm multiple times and then selects the one that generated the lowest inertia. The default value for n_init is 10.\n",
    "\n",
    "- Using K-means with scikit-learn is also very intuitive. As with every algorithm in the library, we have to import the KMeans class from sklearn.cluster and instantiate an object. We can use the n_clusters parameter to set the number of clusters. However, if we choose not to, the default is n_clusters=8.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "​\n",
    "model = KMeans(n_clusters=5)\n",
    "```\n",
    "\n",
    "K-means doesn't need to be trained as a classification, regression, or recommendation algorithm; therefore, we don't need to use the fit and then predict methods. Instead, we can directly call the fit_predict method, which does all the work and returns an array with the cluster for each data point.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "​\n",
    "model = KMeans(n_clusters=5)\n",
    "clusters = model.fit_predict()\n",
    "```\n",
    "\n",
    "After we fit_predict the model, we can access several attributes from the model object:\n",
    "\n",
    "    - model.inertia_: the inertia resulting from the clusters split.\n",
    "    - model.cluster_centers_: the coordinates of the final centroids.\n",
    "    - model.n_iter_: the number of iterations necessary to converge into the resulting clusters.\n",
    "    - model.n_features_in_: the number of features passed to the model.\n",
    "    - model.feature_names_in_: the name of the features passed to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "Import the `KMeans` class from sklearn.cluster.\n",
    "\n",
    "Read the m`all_customers.csv` file into a DataFrame named customers.\n",
    "\n",
    "Instantiate an object of the `KMeans`, and assign the result back to `model`. Use `n_clusters=5`.\n",
    "\n",
    "Fit and predict the model using only the two columns we used in the previous lesson. Assign the result back to `clusters`.\n",
    "\n",
    "Print the clusters as well as the other attributes listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "customers = pd.read_csv(\"mall_customers.csv\")\n",
    "cols_to_keep = ['Annual Income', 'Spending Score']\n",
    "\n",
    "model = KMeans(n_clusters = 5)\n",
    "clusters = model.fit_predict(customers[cols_to_keep])\n",
    "\n",
    "print(clusters)\n",
    "print(model.inertia_)\n",
    "print(model.cluster_centers_)\n",
    "print(model.n_iter_)\n",
    "print(model.n_features_in_)\n",
    "print(model.feature_names_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Elbow Rule with Scikit-Learn\n",
    "\n",
    "Even though we're now using scikit-learn to segment the customers, we still need the elbow curve to determine the best number of clusters.\n",
    "\n",
    "By using scikit-learn, we don't need to calculate the inertia because we can simply access it after fit_predict the algorithm, which means that all we need to do is to initialize the KMeans class multiple times for different numbers of clusters and store the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "1. Write a function called plot_elbow_curve with the following characteristics:\n",
    "\n",
    "    - It takes a DataFrame and a max_clusters parameter representing the highest number of clusters in the curve. Set this parameter to default 10.\n",
    "\n",
    "    - It initializes an empty list called inertias.\n",
    "\n",
    "    - It loops from one to max_clusters and, for each iteration, does the following:\n",
    "\n",
    "        - It instantiates a KMeans object with a new n_clusters value.\n",
    "        - It calls the fit_predict method.\n",
    "        - It appends model.inertia_ to the inertias list.\n",
    "    - After, the loop, use the inertias list to plot the Elbow Curve.\n",
    "\n",
    "    - Return inertias.\n",
    "\n",
    "2. Call the function for the same two variables of the DataFrame. Assign the result back to inertias and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['Annual Income', 'Spending Score']\n",
    "\n",
    "\n",
    "def plot_elbow_curve(df, max_clusters=10):\n",
    "    inertias = []\n",
    "\n",
    "    for k in range(1, max_clusters+1):\n",
    "        model = KMeans(n_clusters=k)\n",
    "        cluster = model.fit_predict(df)\n",
    "        inertias.append(model.inertia_)\n",
    "        \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(range(1, max_clusters+1), inertias, marker='o')\n",
    "    plt.xticks(ticks=range(1, max_clusters+1), labels=range(1, max_clusters+1))\n",
    "    plt.title('Inertia vs Number of Clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return inertias\n",
    "  \n",
    "\n",
    "inertias = plot_elbow_curve(customers[cols_to_keep])\n",
    "print(inertias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "Most machine learning algorithms perform better when they use variables that are scaled to a certain range, and K-means is one of them.\n",
    "\n",
    "The core of the K-means algorithm is the computation of distances between points, either for assigning clusters, calculating centroids, or calculating the inertia generated by the split. Therefore, if the variables in the dataset are of incomparable magnitudes, this could be an issue.\n",
    "\n",
    "Let's say we have a variable that's measured in kilograms and another measured in millimeters. If the data is about human beings, for instance, the values for weight will probably be around dozens of kilograms while the height will be in the thousands. This means the axis in a scatterplot won't be in the same proportion, and points that are more distant in the height axis will be highly penalized.\n",
    "\n",
    "Therefore, we need to bring all the features to a similar scale:\n",
    "\n",
    "![](https://s3.amazonaws.com/dq-content/744/3.1-m744.svg)\n",
    "\n",
    "For this, scikit-learn has the StandardScaler class that performs the standardization of the data.\n",
    "\n",
    "The standardization is done by calculating the z-score for each observation in a column. \n",
    "\n",
    "Just like the KMeans class, using the StandardScaler is also very intuitive. After we instantiate an object, we call fit and then transform and assign the result back to a variable:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "​\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "​\n",
    "df_scaled = scaler.transform(df)\n",
    "```\n",
    "\n",
    "The returned object is a Numpy ndarray. If we print its first five rows, we'll see something like this:\n",
    "\n",
    "```\n",
    "print(type(df_scaled))\n",
    "print(df_scaled[:5])\n",
    "```\n",
    "\n",
    "```\n",
    "numpy.ndarray\n",
    "​\n",
    "array([[-1.73899919, -0.43480148],\n",
    "       [-1.73899919,  1.19570407],\n",
    "       [-1.70082976, -1.71591298],\n",
    "       [-1.70082976,  1.04041783],\n",
    "       [-1.66266033, -0.39597992]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "Write a function called scaler that we'll use to standardize data from now on. This function should receive a DataFrame and do the following:\n",
    "\n",
    "Instantiate an object of the StandardScaler class\n",
    "\n",
    "Pass the DataFrame to the fit method\n",
    "\n",
    "Pass the DataFrame to the transform method\n",
    "\n",
    "Return the scaled data\n",
    "\n",
    "Call the function passing the two columns we've been using from the customers DataFrame. Assign the result back to scaled_customers.\n",
    "\n",
    "Call the plot_elbow_curve function with the scaled data, assign the result back to inertias, and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "cols_to_keep = ['Annual Income', 'Spending Score']\n",
    "\n",
    "\n",
    "def scaler(df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df)\n",
    "\n",
    "    df_scaled = scaler.transform(df)\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "scaled_customers = scaler(customers[cols_to_keep])\n",
    "inertias = plot_elbow_curve(scaled_customers)\n",
    "\n",
    "print(inertias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate K-Means\n",
    "On the previous screen, we scaled the data before plotting the elbow curve. As you probably noticed, the shape of the curve is the same, which means that the decreased rate of inertia didn't suffer big changes. However, the values of inertia are lower, which makes total sense since we now have lower values in the dataset.\n",
    "\n",
    "Now that we understand how to use scikit-learn for clustering, we'll build on that to start segmenting the data using all the variables available and not only two.\n",
    "\n",
    "However, once we change or add more variables, it's necessary to plot the elbow curve again as it will be different from the last.\n",
    "\n",
    "Also, notice that two of our columns need to be resolved:\n",
    "\n",
    "CustomerID: this is the unique identifier for each customer and shouldn't be used for machine learning.\n",
    "\n",
    "Gender: this is a categorical column that is stored as text. We'll need to convert it into a numerical column.\n",
    "\n",
    "### Instructions\n",
    "- Create a copy of the customers DataFrame called customers_modif.\n",
    "\n",
    "- Drop the CustomerID column from customers_modif.\n",
    "\n",
    "- Transform the values in the Gender column into numerical values. Assign 1 to Male and 0 to Female.\n",
    "\n",
    "- Scale the customers_modif DataFrame, and assign the result back to scaled_customers.\n",
    "\n",
    "- Call plot_elbow_curve on scaled_customers.\n",
    "\n",
    "- Analyze the curve and choose the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_modif = customers.copy()\n",
    "customers_modif.drop('CustomerID', axis=1, inplace=True)\n",
    "customers_modif['Gender'] = customers_modif['Gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "scaled_customers = scaler(customers_modif)\n",
    "plot_elbow_curve(scaled_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate K-Means\n",
    "You probably had a hard time deciding the best number of clusters from the Elbow Curve you plotted on the previous screen. There wasn't a clear, sharp elbow in the curve.\n",
    "\n",
    "Unfortunately, that happens more than we'd like when segmenting data. If we check the percentage loss of inertia, we'll notice that decrease is much slower than the one we saw in the previous lesson.- \n",
    "\n",
    "\n",
    "Here are some possible solutions:\n",
    "\n",
    "Gather more data.\n",
    "\n",
    "- Look for new features.\n",
    "\n",
    "- Look for business rules or insights that might lead to a decision for one cluster over another.\n",
    "\n",
    "- Try different numbers of clusters, and see how the split goes in each of them.\n",
    "\n",
    "The best possible candidates here are five, six, and seven clusters. For the sake of learning, we'll choose six and move on to the split and then interpret the results.\n",
    "\n",
    "### Instructions\n",
    "- The customers DataFrame is available for you to use.\n",
    "\n",
    "- Initialize a KMeans object with n_clusters=6, and assign the result back to model.\n",
    "\n",
    "- Pass scaled_customers to fit_predict. Assign the result back to clusters.\n",
    "\n",
    "- Create a new Cluster column in customers using clusters. Number the clusters from 1 to 6.\n",
    "\n",
    "- Print the number of occurrences of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_customers = scaler(customers_modif)\n",
    "\n",
    "\n",
    "model = KMeans(n_clusters=6)\n",
    "clusters = model.fit_predict(scaled_customers)\n",
    "\n",
    "customers['Cluster'] = clusters + 1\n",
    "\n",
    "print(customers['Cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Results — Numerical Variables\n",
    "\n",
    "We have segmented the data on the previous screen, and we can see three clusters have a very similar number of occurrences while the other three are a bit smaller.\n",
    "\n",
    "Now, it's time to interpret the results and summarize the characteristics of each cluster and differentiate them from each other based on the variables used for the segmentation.\n",
    "\n",
    "First, we'll analyze the numerical variables and see how they behave in each cluster. For instance, we want to find information like the following:\n",
    "\n",
    "- Cluster 1 has, on average, a very high income.\n",
    "\n",
    "- Cluster 5 has, on average, the youngest clients.\n",
    "\n",
    "### Instructions\n",
    "- Loop through the list containing the numerical columns, and, for each column, do the following:\n",
    "\n",
    "- Use the groupby method to group each column in customers by the Cluster column, and take the average.\n",
    "\n",
    "- Plot a bar chart for each column where each bar is the average of the column for a cluster.\n",
    "\n",
    "### Questions\n",
    "Which of the following sentences is correct?\n",
    "\n",
    "- On average, customers in cluster 2 and 3 are older.\n",
    "|\n",
    "- The annual income is higher in clusters 5 and 6.\n",
    "\n",
    "- The customers in cluster 5 are, on average, the youngest ones.\n",
    "\n",
    "- Cluster 6 has the lowest average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "numeric_columns = ['Age', 'Annual Income', 'Spending Score']\n",
    "print('m')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    df_plot = customers.groupby('Cluster')[column].mean()\n",
    "    ax = fig.add_subplot(2, 2, i+1)\n",
    "    ax.bar(df_plot.index, df_plot, color=sns.color_palette('Set1'), alpha=0.6)\n",
    "    ax.set_title(f'Average {column.title()} per Cluster', alpha=0.5)\n",
    "    ax.xaxis.grid(False)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Results — Numerical Variables, Part 2\n",
    "\n",
    "The conclusions from the previous screen enable us to understand each cluster and then use them to make business decisions. For instance, customers in cluster 6 have a very high income but the lowest spending score. This means that they could've spent more money in our company.\n",
    "\n",
    "With this kind of information, the marketing team could develop a campaign focused on this cluster specifically and try to incite those customers to spend more money.\n",
    "\n",
    "However, there's more for us to investigate. Scatter plots are a great tool for understanding variable distribution between clusters. To plot these charts, seaborn.scatterplot is a great tool because it easily allows us to plot the data grouped by a variable through the parameter hue.\n",
    "\n",
    "In the previous lesson, we ended up with a chart like this:\n",
    "\n",
    "![](https://s3.amazonaws.com/dq-content/744/scatter1.png)\n",
    "\n",
    "Since we were using only two variables for clustering, this plot alone tells us a lot:\n",
    "\n",
    "Cluster 1 has a high Spending Score but low Annual Income.\n",
    "\n",
    "Cluster 4 has a high Spending Score but a high Annual Income.\n",
    "\n",
    "Now that we have three numerical variables, we'll need more plots to be able to see the entire picture.\n",
    "\n",
    "### Instructions\n",
    "- Use seaborn.scatterplot to plot the Annual Income over the Age. Pass the Cluster column to hue.\n",
    "\n",
    "- Use seaborn.scatterplot to plot the Spending Score over the Age. Pass the Cluster column to hue.\n",
    "\n",
    "- Use seaborn.scatterplot to plot the Spending Score over the Annual Income. Pass the Cluster column to hue.\n",
    "\n",
    "- Can you see some new characteristics for a cluster that you couldn't see on the previous screen? Can you confirm the conclusion you've drawn on the previous screen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 8))\n",
    "sns.scatterplot(x='Age', y='Annual Income', hue='Cluster', data=customers, palette='Set1', alpha=0.4, ax=axs[0][0])\n",
    "\n",
    "sns.scatterplot(x='Age', y='Spending Score', hue='Cluster', data=customers, palette='Set1', alpha=0.4, ax=axs[0][1], legend=False)\n",
    "\n",
    "sns.scatterplot(x='Annual Income', y='Spending Score', hue='Cluster', data=customers, palette='Set1', alpha=0.4, ax=axs[1][0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Results — Categorical Variables\n",
    "\n",
    "Plots like those on the previous two screens are useful for understanding how numerical variables behave between clusters. While the bar charts show each variable at a time per cluster, the scatter plots enable us to easily understand how two variables relate to each other.\n",
    "\n",
    "For instance, the third plot in the previous lesson shows us that cluster 5 contains customers with high income and high scores, while cluster 4 has a high score but low income.\n",
    "\n",
    "So, now we need to understand how the categorical column Gender impacts the cluster split. Is the cluster with customers from only one gender? Are any clusters equally divided between men and women?\n",
    "\n",
    "Unlike the numerical columns, a scatter plot wouldn't show much value when analyzing a categorical column, even if you transformed the text value to numeric again. Therefore, our goal is to visualize the percentage of each category (male and female) in each one of our six clusters, like so:\n",
    "\n",
    "Cluster 3 has 15% of men and 85% of women.\n",
    "To achieve that, we'll take advantage of the pandas.crosstab function. This function performs cross-tabulation in order to compute the \n",
    "frequency of a variable given another one, like the example below:\n",
    "\n",
    "```python\n",
    "plot_df = pd.crosstab(\n",
    "  index=customers['Cluster'], columns=customers['Gender'],\n",
    "  values=customers['Gender'], aggfunc='size', normalize='index'\n",
    ")\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "index: the values to be grouped in the rows\n",
    "\n",
    "columns: the values to be grouped in the columns\n",
    "\n",
    "values: the values to be aggregated given an aggregation function\n",
    "\n",
    "aggfunc='size': the aggregation function — the size method returns the number of occurrences\n",
    "\n",
    "normalize='index': the values will be normalized over each row, and we'll see the percentage value\n",
    "\n",
    "This is a sample of the outcome:\n",
    "\n",
    "Cluster\tFemale\tMale\n",
    "4\t1\t0\n",
    "5\t0.875\t0.125\n",
    "6\t0.451613\t0.548387\n",
    "By looking at this outcome, we can already see some patterns. However, if we had more columns, it would be much more difficult to evaluate all of them. That's why we want to visualize these results in a plot.\n",
    "\n",
    "### Instructions\n",
    "Use the pandas.crosstab function to calculate the percentage of male and female customers in each cluster. Assign the result back to plot_df.\n",
    "\n",
    "Use plot_df to plot a stacked bar chart of percentages of each cluster?\n",
    "\n",
    "Questions\n",
    "1. Select all the correct sentences:\n",
    "\n",
    "- 4 out of the 6 clusters only contain people from one gender.\n",
    "\n",
    "- The gender is well divided inside every cluster.\n",
    "\n",
    "- Cluster 6 is the only cluster without a dominant gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.crosstab(\n",
    "  index=customers['Cluster'], columns=customers['Gender'],\n",
    "  values=customers['Gender'], aggfunc='size', normalize='index'\n",
    ")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "plot_df.plot.bar(stacked=True, ax=ax, alpha=0.6)\n",
    "ax.set_title(f'% Gender per Cluster', alpha=0.5)\n",
    "\n",
    "ax.set_ylim(0, 1.4)\n",
    "ax.legend(frameon=False)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "Now that we've analyzed how each variable behaves between clusters, combine all the conclusions and write a couple of lines describing each cluster based on the distributions of all variables for each one of them. This is an important step in order to understand the characteristics of each cluster and how to take advantage of them.\n",
    "\n",
    "During this course, we accomplished the following:\n",
    "\n",
    "We covered the basics of unsupervised machine learning and explored the K-means algorithm.\n",
    "\n",
    "We built our own version of this algorithm.\n",
    "\n",
    "We covered the concepts of Euclidean Distance and Inertia, and we learned how to calculate centroids coordinates as well as plot the elbow curve and choose the number of clusters.\n",
    "\n",
    "Next, we have a guided project for you that asks you to practice everything you've learned throughout these lessons."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
